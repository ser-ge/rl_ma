{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_assignment_2_2021.ipynb",
      "provenance": [
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1582541397384
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1581969444858
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1581964637124
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1581957222796
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2.ipynb?workspaceId=mtthss:ucl::citc",
          "timestamp": 1581683857481
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2.ipynb",
          "timestamp": 1581608852108
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_2.ipynb",
          "timestamp": 1580904796770
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_2.ipynb",
          "timestamp": 1548782473207
        },
        {
          "file_id": "1t1PIXqa3m-irLvQ_fQ7qgaBZeTNqiG2v",
          "timestamp": 1542801802962
        },
        {
          "file_id": "1Ldj742iIDtvjYKKwENvrpTQ3Hm2wrqIg",
          "timestamp": 1517862636703
        },
        {
          "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
          "timestamp": 1517660129183
        },
        {
          "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
          "timestamp": 1517174839485
        },
        {
          "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
          "timestamp": 1515086437469
        },
        {
          "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
          "timestamp": 1511098107887
        }
      ],
      "collapsed_sections": [
        "qB0tQ4aiAaIu"
      ],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook",
        "kind": "private"
      },
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL homework 2\n",
        "**Due date: 30th March 2021, 4:00pm **\n",
        "\n",
        "Name: ***put your name here***\n",
        "\n",
        "Student number: ***put your student number here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sns0IKYNtsA"
      },
      "source": [
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<student_id>_ucldm_rl2.ipynb`** before the deadline above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "## Context\n",
        "\n",
        "In this assignment, we will take a first look at learning algorithms for sequential decision problems.\n",
        "\n",
        "## Background reading\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 3 - 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use Python to implement several reinforcement learning algorithms.\n",
        "\n",
       "You will then run these algorithms on a few problems, to understand their properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB0tQ4aiAaIu"
      },
      "source": [
        "### Import Useful Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "source": [
        "import matplotlib.collections as mcoll\n",
        "import matplotlib.path as mpa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NDhSYfSDcCC"
      },
      "source": [
        "### Set options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "source": [
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-colorblind')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrRR76eAd6u"
      },
      "source": [
        "### Some grid worlds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "source": [
        "W = -100  # wall\n",
        "G = 100  # goal\n",
        "\n",
        "GRID_LAYOUT = np.array([\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W],\n",
        "  [W, W, 0, W, W, W, W, W, W, 0, W, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, G, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, W, 0, 0, 0, 0, 0, 0, 0, 0, W, W],\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W]\n",
        "])\n",
        "\n",
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = GRID_LAYOUT\n",
        "    self._start_state = (2, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "\n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(self, obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "    \n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    reward = self._layout[new_y, new_x]\n",
        "    if self._layout[new_y, new_x] == W:  # wall\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = -1.\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "\n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 10*np.random.normal(0, width - new_x + new_y)\n",
        "\n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()\n",
        "\n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout != W, interpolation=\"nearest\", cmap='pink')\n",
        "    plt.gca().grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(9, 2, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmHhbuuk8bPm"
      },
      "source": [
        "SMALL_GRID_LAYOUT = np.array([\n",
        "  [W, W, W, W, W],\n",
        "  [W, W, 3, W, W],\n",
        "  [W, W, 0, W, W],\n",
        "  [W, 1, 0, W, W],\n",
        "  [W, W, 0, W, W],\n",
        "  [W, W, 0, 4, W],\n",
        "  [W, W, W, W, W]\n",
        "])\n",
        "\n",
        "\n",
        "def plot_small_grid(version=1):\n",
        "  plt.imshow(SMALL_GRID_LAYOUT < -1, interpolation='nearest', cmap='pink_r',\n",
        "             vmin=-0.2, vmax=1.2)\n",
        "  if version == 1:\n",
        "    plt.text(2, 1, '$+3$', ha='center', va='center', fontsize=12)\n",
        "    plt.text(1, 3, '$+1$', ha='center', va='center', fontsize=12)\n",
        "    plt.text(3, 5, '$+4$', ha='center', va='center', fontsize=12)\n",
        "  else:\n",
        "    plt.text(2, 1, '$+2$', ha='center', va='center', fontsize=12)\n",
        "    plt.text(1, 3, '$+1$', ha='center', va='center', fontsize=12)\n",
        "    plt.text(3, 5, '$+5$', ha='center', va='center', fontsize=12)\n",
        "  plt.text(2, 3, '$S$', ha='center', va='center', fontsize=12)\n",
        "  h, w = SMALL_GRID_LAYOUT.shape\n",
        "  for r in np.arange(0.5, h):\n",
        "    plt.plot([-0.5, w - 0.5], [r, r], '-k', lw=3, alpha=0.4)\n",
        "  for c in np.arange(0.5, w):\n",
        "    plt.plot([c, c], [-0.5, h - 0.5], '-k', lw=3, alpha=0.4)\n",
        "  plt.xticks([]); plt.yticks([]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOu9RZY3AkF1"
      },
      "source": [
        "### Helper functions\n",
        "(You should not have to change, or even look at, these.  Do run the cell to make sure the functions are loaded though.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EttQGJ1n5Zn"
      },
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = grid.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += reward\n",
        "    return mean_reward/float(number_of_steps)\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(grid, values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values - 1000*(grid<0), interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(grid, action_values, vmin=-5, vmax=5):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(4, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(grid, q[..., a], vmin=vmin, vmax=vmax)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(4, 3, 5)\n",
        "  v = np.max(q, axis=-1)\n",
        "  plot_values(grid, v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "  \n",
        "  # Plot arrows:\n",
        "  plt.subplot(4, 3, 11)\n",
        "  plot_values(grid, grid==0, vmax=1)\n",
        "  for row in range(len(grid)):\n",
        "    for col in range(len(grid[0])):\n",
        "      if grid[row][col] == 0:\n",
        "        argmax_a = np.argmax(q[row, col])\n",
        "        if argmax_a == 0:\n",
        "          x = col\n",
        "          y = row + 0.5\n",
        "          dx = 0\n",
        "          dy = -0.8\n",
        "        if argmax_a == 1:\n",
        "          x = col - 0.5\n",
        "          y = row\n",
        "          dx = 0.8\n",
        "          dy = 0\n",
        "        if argmax_a == 2:\n",
        "          x = col\n",
        "          y = row - 0.5\n",
        "          dx = 0\n",
        "          dy = 0.8\n",
        "        if argmax_a == 3:\n",
        "          x = col + 0.5\n",
        "          y = row\n",
        "          dx = -0.8\n",
        "          dy = 0\n",
        "        plt.arrow(x, y, dx, dy, width=0.02, head_width=0.4, head_length=0.4, length_includes_head=True, fc='k', ec='k')\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "\n",
        "def parameter_study(parameter_values, parameter_name,\n",
        "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])\n",
        "\n",
        "\n",
        "def colorline(x, y, z):\n",
        "    \"\"\"\n",
        "    Based on:\n",
        "    http://nbviewer.ipython.org/github/dpsanders/matplotlib-examples/blob/master/colorline.ipynb\n",
        "    http://matplotlib.org/examples/pylab_examples/multicolored_line.html\n",
        "    Plot a colored line with coordinates x and y\n",
        "    Optionally specify colors in the array z\n",
        "    Optionally specify a colormap, a norm function and a line width\n",
        "    \"\"\"\n",
        "    segments = make_segments(x, y)\n",
        "    lc = mcoll.LineCollection(segments, array=z, cmap=plt.get_cmap('copper_r'),\n",
        "                              norm=plt.Normalize(0.0, 1.0), linewidth=3)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.add_collection(lc)\n",
        "    return lc\n",
        "\n",
        "\n",
        "def make_segments(x, y):\n",
        "    \"\"\"\n",
        "    Create list of line segments from x and y coordinates, in the correct format\n",
        "    for LineCollection: an array of the form numlines x (points per line) x 2 (x\n",
        "    and y) array\n",
        "    \"\"\"\n",
        "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "    return segments\n",
        "\n",
        "\n",
        "def plotting_helper_function(_x, _y, title=None, ylabel=None):\n",
        "  z = np.linspace(0, 0.9, len(_x))**0.7\n",
        "  colorline(_x, _y, z)\n",
        "  plt.plot(0, 0, '*', color='#000000', ms=20, alpha=0.7, label='$w^*$')\n",
        "  plt.plot(1, 1, '.', color='#ee0000', alpha=0.7, ms=20, label='$w_0$')\n",
        "  min_y, max_y = np.min(_y), np.max(_y)\n",
        "  min_x, max_x = np.min(_x), np.max(_x)\n",
        "  min_y, max_y = np.min([0, min_y]), np.max([0, max_y])\n",
        "  min_x, max_x = np.min([0, min_x]), np.max([0, max_x])\n",
        "  range_y = max_y - min_y\n",
        "  range_x = max_x - min_x\n",
        "  max_range = np.max([range_y, range_x])\n",
        "  plt.arrow(_x[-3], _y[-3], _x[-1] - _x[-3], _y[-1] - _y[-3], color='k',\n",
        "            head_width=0.04*max_range, head_length=0.04*max_range,\n",
        "            head_starts_at_zero=False)\n",
        "  plt.ylim(min_y - 0.2*range_y, max_y + 0.2*range_y)\n",
        "  plt.xlim(min_x - 0.2*range_x, max_x + 0.2*range_x)\n",
        "  ax = plt.gca()\n",
        "  ax.ticklabel_format(style='plain', useMathText=True)\n",
        "  plt.legend(loc=2)\n",
        "  plt.xticks(rotation=12, fontsize=10)\n",
        "  plt.yticks(rotation=12, fontsize=10)\n",
        "  plt.locator_params(nbins=3)\n",
        "  if title is not None:\n",
        "    plt.title(title, fontsize=20)\n",
        "  if ylabel is not None:\n",
        "    plt.ylabel(ylabel, fontsize=20)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# Section 1: Tabular RL _(60 pts in total for the whole section)_\n",
        "\n",
        "In this section, observations will be states in the environment, so the agent state, environment state, and observation will all be the same, and we will use the word `state` interchangably with `observation`.  You will implement agents, which should be in pure Python - so you cannot use JAX/TensorFlow/PyTorch to compute gradients. Using `numpy` is fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53WqBnBywTHm"
      },
      "source": [
        "## Q1: A small grid world _(10 pts in total)_\n",
        "\n",
        "Consider the grid MDP below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1rDz0bP5J4m"
      },
      "source": [
        "plot_small_grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5ywpx298scu"
      },
      "source": [
        "The agent starts at the state marked $S$, and  can move up, left, right, or down.  Whenever it enters a cell with a positive number, the agent receives that reward and the episode terminates.\n",
        "### Q1.1 [**3 pts**]\n",
        "Use the code below to plot the value of the optimal policy from the starting state, $v_*(S)$, as a function of the discount factor $\\gamma$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prj9_fvXdeET"
      },
      "source": [
        "def v(discount):\n",
        "  return -1.  # This is wrong, replace it with the appropriate calculation\n",
        "\n",
        "# Don't change this function\n",
        "def plot_v():\n",
        "  discounts = np.arange(0, 1.001, 0.01)\n",
        "  plt.plot(discounts, [v(g) for g in discounts])\n",
        "  plt.xlabel('$\\gamma$')\n",
        "  plt.ylabel('$v_*(S)$')\n",
        "\n",
        "plot_v()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Igl6g0Sgw2v"
      },
      "source": [
        "### Q1.2 [**2 pts**]\n",
        "\n",
        "Specify the optimal policy, as a function of $\\gamma$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcinsTpmmmkW"
      },
      "source": [
        "### Answer\n",
        "\n",
        "_(Double click here to write your answer)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywAFQua4hL-U"
      },
      "source": [
        "### Q1.3 [**2 pts**]\n",
        "Consider the variation of this problem shown in the next plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPkHt2P8hsBc"
      },
      "source": [
        "plot_small_grid(version=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFB_JRgBhwB9"
      },
      "source": [
        "Suppose a behavioural scientist was doing an experiment where they gave rewards to an animal.  Suppose the setting was as depicted above, where the rewards for instance correspond to food or some other kind of treat (e.g., 5 food pellets at the location marked $+5$). It turns out that, after repeatedly exploring the grid, the animal seems to prefer going up to the reward of $+2$.\n",
        "\n",
        "Prove that in the MDP depicted above no scalar discount $\\gamma \\in [0, 1]$ exists for which the optimal policy is to go to $+2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzF3dp9Piwsu"
      },
      "source": [
        "### Answer\n",
        "\n",
        "_(Double click here to write your answer)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvGKvIYRiLvq"
      },
      "source": [
        "### Q1.4 [**3 pts**]\n",
        "In the same setting as the previous question, now consider the following generalised definition of a Monte Carlo return\n",
        "\\begin{align*}\n",
        "G_t\n",
        "& = R_{t+1} + f(R_{t+2} + f(R_{t+3} + f(\\ldots))) \\\\\n",
        "& = R_{t+1} + f(G_{t+1}) \\,,\n",
        "\\end{align*}\n",
        "In this formulation, we get standard discounting when we define $f(x) = \\gamma x$. Consider the following alternative where instead of multiplying with a factor $\\gamma$, we raise the value to the power: $f(x) = x^\\gamma$.  Does this mathematical model better explain the observed behaviour, in the sense that a $\\gamma$ exists for which the optimal policy goes to $+2$?  If so, give such a value for $\\gamma$, and prove that the policy of going to $+2$ is optimal with that choice.  If not, prove that no such $\\gamma$ exists.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLGbEPzJi0hH"
      },
      "source": [
        "### Answer\n",
        "\n",
        "_(Double click here to write your answer)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGTgvDm6wTRg"
      },
      "source": [
        "# Implementing algorithms\n",
        "\n",
        "**Minimal agent interface**:\n",
        "\n",
        "Each agent should implement the following methods:\n",
        "\n",
        "### `__init__(self, number_of_actions, number_of_states, initial_observation, ...)`:\n",
        "The constructor will provide the agent the number of actions, number of states, and the initial state. You can get the initial state by first instatiating an environment, using `grid = Grid()`, and then calling `grid.get_obs()`. The constructor may accept additional arguments (denoted here by the ellipsis `...`) depending on the specific algorithm implemented by the agent.\n",
        "\n",
        "### `step(self, reward, discount, next_state)`:\n",
        "\n",
        "The step method of an agent should update its internals, in whatever way is appropriate for its learning algorithm, and return a new action to be executed in the environment.\n",
        "\n",
        "When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_state` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_state})$\" in the update, because $\\gamma = 0$ (for whatever definition of $v$ is appropriate---for instance, $v(s)$ could be defined in terms of action values estimates that we are learning, for instance by $v(s) = \\max_a q(s, a)$).  Therefore, the end of an episode can be seamlessly handled with the same step function.\n",
        "\n",
        "**Note on the initial action**:\n",
        "\n",
        "Some algorithms (Q-learning, Sarsa) need to remember the last action in order to update its value when they see the next state and reward.  In the constructor (`__init__`), make sure you set the initial action to zero, e.g.,\n",
        "```\n",
        "def __init__(...):\n",
        "  (...)\n",
        "  self._last_action = 0\n",
        "  (...)\n",
        "```\n",
        "In our experiments the helper functions above will execute the action `0` (which corresponds to `up` in the grid world) as the initial action to begin the run loop of the experiment.  This initial action is only executed once, and the beginning of the very first episode---not at the beginning of each episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Z5IgXfU2Qw"
      },
      "source": [
        "### A random agent\n",
        "\n",
        "Below we show a reference implementation of a simple random agent, implemented according to the interface above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf64o3b3U6A4"
      },
      "source": [
        "class Random(object):\n",
        "\n",
        "  def __init__(self, number_of_actions, number_of_states, initial_state):\n",
        "    self._number_of_actions = number_of_actions\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    next_action = np.random.randint(self._number_of_actions)\n",
        "    return next_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaGeLcsvixmt"
      },
      "source": [
        "### The grid\n",
        "\n",
        "The cell below shows the `Grid` environment that we will use in this section. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-100` for bumping into a wall, `+100` for reaching the goal, and `-1` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlFuWFzIi5uB"
      },
      "source": [
        "grid = Grid()\n",
        "grid.plot_grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8oKd0oyvNcH"
      },
      "source": [
        "\n",
        "## Q2: Implement TD learning **[5 pts]**\n",
        "Implement an agent that acts randomly, and _on-policy_ estimates state values $v(s)$, using one-step TD learning with step size $\\alpha=0.1$.\n",
        "\n",
        "In addition to the base interface, also implement a property `state_values(self)` returning the vector of all state values (one value per state).\n",
        "\n",
        "You should be able to use the `__init__` as provided below, so you just have to implement the `step` function.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the TD error when the first transition has occurred.  \n",
        "\n",
        "**Hint**: in the `step` you similarly will want to store the previous state to be able to compute the next TD error on the next step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yKt1qYYjWTR"
      },
      "source": [
        "class RandomTD(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
        "    self._values = np.zeros(number_of_states)\n",
        "    self._state = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "\n",
        "  @property\n",
        "  def state_values(self):\n",
        "    return self._values\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaMmp1lDgpUG"
      },
      "source": [
        "### Run the next cell to run the `RandomTD` agent on a grid world.\n",
        "\n",
        "If everything worked as expected, the plot below will show the estimates state values under the random policy. This includes values for unreachable states --- on the walls and on the goal (we never actually reach the goal --- rather, the episode terminates on the transition to the goal.  The values on the walls and goal are, and will always remain, zero (shown in orange below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0ZoYwgZfho2"
      },
      "source": [
        "# Do not modify this cell.\n",
        "agent = RandomTD(grid._layout.size, 4, grid.get_obs())\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "v = agent.state_values\n",
        "plot_values(GRID_LAYOUT, v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-300, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxc_Sx7og4JH"
      },
      "source": [
        "## Q3: Policy iteration **[5 pts]**\n",
        "We used TD to do policy evaluation for the random policy on this problem.  Consider doing policy improvement, by taking the greedy policy with respect to a one-step look-ahead.  For this, you may assume we have a true model, so for each state and for each action we can look at the value of the resulting state, and would then pick the action with the highest reward plus subsequent state value. In other words, you can assume we can use $q(s, a) = \\mathbb{E}[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s, A_t = a]$, where $v$ is the value function learned by TD as implemented. Then we consider the policy that picks the action with the highest action value $q(s, a)$. You do **not** have to implement this, just answer the following question.\n",
        "\n",
        "The above amounts to performing an iteration of policy evaluation and policy improvement.  If we repeat this process over and over again, and repeatedly evaluate the greedy policy and then perform an improvement step by picking the greedy policy, would the policy eventually become optimal?  Explain why or why not in at most three sentences.\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfA7ifHvO-M"
      },
      "source": [
        "\n",
        "## Q4: Implement a general Q-learning agent **[15 pts]**\n",
        "Implement a  **general Q-learning** agent that learns action values from experience.   The agent must act according to an $\\epsilon$-greedy policy over its action values.  It must be configurable so as to implement any of **Sarsa**, **Expected Sarsa**, **Q-learning**,  and **double Q-learning**.\n",
        "\n",
        "The `__init__` must accept two functions `target_policy` and `behaviour_policy` as arguments.   The function `behaviour_policy(action_values)` should map `action_values` to a single action. \n",
        "\n",
        "For instance, the random policy can be implemented as:\n",
        "```\n",
        "def behaviour_policy(action_values):\n",
        "  return np.random.randint(len(action_values))\n",
        "```\n",
        "\n",
        "The target policy is defined by a function `target_policy(action_values, action)`, which returns **a vector** with one probability per action.  The `action` argument is used for instance by Sarsa (where the target policy is a greedy distribution with its peak on the selected action).\n",
        "\n",
        "For instance, the random target policy is:\n",
        "```\n",
        "def target_policy(action_values, unused_action):\n",
        "  number_of_actions = len(action_values)\n",
        "  return np.ones((number_of_actions,))/number_of_actions\n",
        "```\n",
        "\n",
        "The `__init__` must also accept  a `double` boolean flag. Note that this is compatible with any choice of `target_policy` and `behaviour_policy`. For instance, if the `target_policy` is the policy described above for Sarsa and `double=True`, the algorithm should implement **double Sarsa**. Note that we then need two action-value functions.\n",
        "\n",
        "You should be able to use the `__init__` as provided below, so you just have to implement the `step` function.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the update on the first transition in the agent's lifetime.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEuPtZVwjtq2"
      },
      "source": [
        "class GeneralQ(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state,\n",
        "               target_policy, behaviour_policy, double, step_size=0.1):\n",
        "    # Settings.\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._target_policy = target_policy\n",
        "    self._double = double\n",
        "    # Initial state.\n",
        "    self._s = initial_state\n",
        "    # Tabular q-estimates.\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    if double:\n",
        "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
        "    # The first action in an agent's lifetime is always 0(=up) in our setup.\n",
        "    self._last_action = 0\n",
        "\n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return (self._q + self._q2)/2 if self._double else self._q\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkk8sMxE0N4"
      },
      "source": [
        "## Experiment 1: Run the cells below\n",
        "\n",
        "This will train Q-learning, Sarsa, Expected Sarsa, and double Q-learning agents on the deterministic version of the Grid problem.\n",
        "\n",
        "The agents will be trained  with a step size $\\alpha=\\frac{1}{10}$ and $\\epsilon$-greedy behaviour, with $\\epsilon=\\frac{1}{4}$.\n",
        "\n",
        "The plots will show action values for each of the actions, as well as a state value defined by $v(s) = \\max_a q(s, a)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlsvLKxYRA27"
      },
      "source": [
        "# Agent settings.\n",
        "# Do not modify this cell.\n",
        "epsilon = 0.25\n",
        "step_size = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "GsNBHNZtHCPe"
      },
      "source": [
        "# Q-learning\n",
        "# Do not modify this cell.\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  return np.eye(len(q))[np.argmax(q)]\n",
        "\n",
        "learned_qs = []\n",
        "for _ in range(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=False, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-20, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ26VmlsSSmw"
      },
      "source": [
        "# Sarsa\n",
        "# Do not modify this cell.\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  return np.eye(len(q))[a]\n",
        "\n",
        "learned_qs = []\n",
        "for _ in range(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=False, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-20, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mti_FtEeSaqH"
      },
      "source": [
        "# Expected Sarsa\n",
        "# Do not modify this cell.\n",
        "grid = Grid()\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  greedy = np.eye(len(q))[np.argmax(q)]\n",
        "  return greedy - greedy*epsilon + epsilon/4 \n",
        "\n",
        "learned_qs = []\n",
        "for _ in range(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=False, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-20, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DwHqxeZ1rXa"
      },
      "source": [
        "# Double Q-learning\n",
        "# Do not modify this cell.\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  # Place equal probability on all actions that achieve the `max` value.\n",
        "  # This is equivalent to `return np.eye(len(q))[np.argmax(q)]` for Q-learning\n",
        "  # But results in slightly lower variance updates for double Q.\n",
        "  max_q = np.max(q)\n",
        "  pi = np.array([1. if qi == max_q else 0. for qi in q])\n",
        "  return pi / sum(pi)\n",
        "\n",
        "learned_qs = []\n",
        "for _ in range(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=True, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-50, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGptHwE23lmP"
      },
      "source": [
        "## Q5: Analyse results _(10 pts in total)_\n",
        "\n",
        "Consider the greedy policy with respect to the estimated values learned by each of the four agents.\n",
        "\n",
        "**[5 pts]** How and why do the policies found by Q-learning, Sarsa, Expected Sarsa, and double Q-learning differ? Explain notable qualitative differences in at most four sentences.\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "**[5 pts]** Which of the algorithms, out of Q-learning, Sarsa, Expected Sarsa, and double Q-learning with the learning parameters (exploration, step size) as discussed above, will *in general* yield higher returns on average during learning?  You are allowed to specify a partial (rather than a full) ordering over the algorithms, but try to be as specfic as you can. Explain your answer in at most four sentences.\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK0ExHgqn8eQ"
      },
      "source": [
        "## Experiment 2: Run the cells below\n",
        "\n",
        "This will train Q-learning and double Q-learning agents on the stochastic version of the Grid problem.\n",
        "\n",
        "In the noisy version, a zero-mean Gaussian is added to all rewards.  \n",
        "\n",
        "The variance of this noise is higher the further to the left you go, and the further down.\n",
        "\n",
        "The agents will be trained  with a step size $\\alpha=\\frac{1}{10}$ and $\\epsilon$-greedy behaviour, with $\\epsilon=\\frac{1}{4}$.\n",
        "\n",
        "The plots will show action values for each of the actions, as well as a state value defined by $v(s) = \\max_a q(s, a)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LfLSB5Gn3Q6"
      },
      "source": [
        "# Agent settings.\n",
        "# Do not modify this cell.\n",
        "epsilon = 0.25\n",
        "step_size = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml8AmcN3nrvJ"
      },
      "source": [
        "# Q-learning and double Q-learning.\n",
        "# Do not modify this cell.\n",
        "\n",
        "def target_policy(q, a):\n",
        "  max_q = np.max(q)\n",
        "  pi = np.array([1. if qi == max_q else 0. for qi in q])\n",
        "  return pi / sum(pi)\n",
        "\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "\n",
        "mean_reward_q_learning = []\n",
        "mean_reward_double_q_learning = []\n",
        "for _ in range(30):\n",
        "  grid = Grid(noisy=True)\n",
        "  q_agent = GeneralQ(\n",
        "      grid._layout.size, 4, grid.get_obs(),\n",
        "      target_policy, behaviour_policy, double=False, step_size=step_size)\n",
        "  dq_agent = GeneralQ(\n",
        "      grid._layout.size, 4, grid.get_obs(),\n",
        "      target_policy, behaviour_policy, double=True, step_size=step_size)\n",
        "  mean_reward_q_learning.append(run_experiment(grid, q_agent, int(2e5)))\n",
        "  mean_reward_double_q_learning.append(run_experiment(grid, dq_agent, int(2e5)))\n",
        "\n",
        "plt.violinplot([mean_reward_q_learning, mean_reward_double_q_learning])\n",
        "plt.xticks([1, 2], [\"Q-learning\", \"Double Q-learning\"], rotation=60, size=12)\n",
        "plt.ylabel(\"average reward during learning\", size=12)\n",
        "ax = plt.gca()\n",
        "ax.grid(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Jj_RoHbxfF"
      },
      "source": [
        "## Q6: Analyse results **[8 pts]**\n",
        "\n",
        "Which among Q-learning and Double Q-learning has a higher average reward? Explain why, discussing at least a) the dynamics of the algorithm, b) how this affects behaviour, and c) why the behaviour yields higher rewards for one algorithm rather than the other. Be concise.\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr71oDDAsxTT"
      },
      "source": [
        "## Q7: Target Q-learning **[7 pts]**\n",
        "\n",
        "Consider a new algorithm which we will refer to as **target Q-learning**. The target Q-learning algorithm applies Q-learning updates in the form \n",
        "$\\ \\ q(S_t, A_t) \\leftarrow R_{t+1} + \\gamma \\max_a q'(S_{t+1}, a) \\ \\ $  \n",
        "where the values $q'$ have been pre-trained by running $\\epsilon$-greedy Expected Sarsa, and are then held fixed throughout training.\n",
        "\n",
        "We can now imagine now to train both Q-learning and target Q-learning on the deterministic Grid problem.  In both Q-learning and target Q-learning the behaviour policy will select actions based on the same $\\epsilon$-greedy policy that was used to pre-train the $q'$ values, but using the current action-value estimates (not the fixed target values $q'$).\n",
        "\n",
        "Explain concisely which of the two algorithms will perform better and why. (If you're tempted to answer 'it depends', be sure to be clear what you think it depends on, and why. Don't be vague: point could be subtracted for including irrelevant or false statements, even if the correct answer is also given.)\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R58NP87jbM"
      },
      "source": [
        "# Section 2: Off-policy Bellman operators with function approximation\n",
        "# _(40 pts total for the whole section)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIyALFeoiov9"
      },
      "source": [
        "## Q8: Bellman operator for prediction **[10 pts]**\n",
        "\n",
        "We are going to implement a Bellman operator for a simple MDP. You should:\n",
        "\n",
        "\n",
        "1. Write a value function `v(w, x)` that outputs a linear value estimate when we have weights `w` (a numpy vector) and feature vector `x` (a numpy vector of the same size).\n",
        "2. Write an operator function `T(w, pi, mu, l, g)` that takes weights `w`, a target policy `pi`, a behaviour policy `mu`, a trace parameter `l`, and a discount `g`, and outputs an off-policy-corrected lambda-return.  For this question, implement the standard importance-weighted per-decision lambda-return. There will only be two actions, with the same policy in each state, so we can define `pi` to be a number which is the target probability of selecting action `a` in any state (s.t. `1 - pi` is the probability of selecting `b`), and similarly for the behaviour `mu`.\n",
        "3. Write an expected weight update, that uses the above two functions to compute the **expected** weight update.  The expectation should take into account the probabilities of actions in the future, as well as the steady-state (=long-term) probability of being in a state.  The step size of the update should be $\\alpha=0.1$.\n",
        "\n",
        "There are two states, $s_1$ and $s_2$.  All rewards are zero, and therefore can be ignored.  The state features $x_1 = x(s_1)$ and $x_2 = x(s_2)$ for the two states are $x_1 = [1, 1]^{\\top}$ and $x_2 = [2, 1]^{\\top}$.  In each state, there are two actions, $a$ and $b$.  Action $a$ always transitions to state $s_1$, action $b$ always transitions to state $s_2$.\n",
        "\n",
        "![MDP](https://hadovanhasselt.files.wordpress.com/2020/02/mdp.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEpxlyUtrj3i"
      },
      "source": [
        "# state features (do not change)\n",
        "x1 = np.array([1., 1.])\n",
        "x2 = np.array([2., 1.])\n",
        "\n",
        "def v(w, x):\n",
        "  pass\n",
        "\n",
        "def T(w, pi, mu, l, g):\n",
        "  pass\n",
        "\n",
        "def expected_update(w, pi, mu, l, g, lr):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U42IsCPW0KGY"
      },
      "source": [
        "##Experiment 3: run the cell below\n",
        "The cell below runs an experiment, across different target policies and trace parameters $\\lambda$.\n",
        "\n",
        "The plots below the cell will show how the weights move within the 2-dimensional weight space, starting from $w_0 = [1, 1]^{\\top}$ (shown as red dot).  The optimal solution $w_* = [0, 0]^{\\top}$ is also shown (as black star)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTFFpQSX0Eaj"
      },
      "source": [
        "def generate_ws(w, pi, mu, l, g):\n",
        "  \"\"\"Apply the expected update 1000 times\"\"\"\n",
        "  ws = [w]\n",
        "  for _ in range(1000):\n",
        "    w = w + expected_update(w, pi, mu, l, g, lr=0.1)\n",
        "    ws.append(w)\n",
        "  return np.array(ws)\n",
        "\n",
        "mu = 0.5  # behaviour\n",
        "g = 0.99  # discount\n",
        "\n",
        "lambdas = np.array([0, 0.8, 0.9, 0.95, 1.])\n",
        "pis = np.array([0., 0.1, 0.2, 0.5, 1.])\n",
        "\n",
        "fig = plt.figure(figsize=(22, 17))\n",
        "fig.subplots_adjust(wspace=0.25, hspace=0.3)\n",
        "\n",
        "for r, pi in enumerate(pis):\n",
        "  for c, l in enumerate(lambdas):\n",
        "    plt.subplot(len(pis), len(lambdas), r*len(lambdas) + c + 1)\n",
        "    w = np.ones_like(x1)\n",
        "    ws = generate_ws(w, pi, mu, l, g)\n",
        "    title = '$\\\\lambda={:1.3f}$'.format(l) if r == 0 else None\n",
        "    ylabel = '$\\\\pi={:1.1f}$'.format(pi) if c == 0 else None\n",
        "    plotting_helper_function(ws[:, 0], ws[:, 1], title, ylabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KxL4o357_dt"
      },
      "source": [
        "## Q9: Analyse results (30 pts total)\n",
        "1. **[2 pts]** How many of the above 25 experiments diverge?\n",
        "1. **[4 pts]** Why are the results asymmetric across different $\\pi$?  In particular, explain why the results look different when comparing $\\pi = \\pi(a | \\cdot) = 0$ to $\\pi(a | \\cdot) = 1$.\n",
        "1. **[4 pts]** For which policies $\\pi$, is the true value function $v_{\\pi}$ representable in the above feature space (spanned by $x_1, x_2$).\n",
        "1. **[4 pts]** For which combination of $\\pi(a)$ and $\\lambda$ does the expected update (with uniform random behaviour) converge? (Do not limit the answer to the subset of values in the plots above, but to all choices of $\\lambda$ and $\\pi$, but do restrict yourself to state-less policies, as above, for which the action probabilities are equal in the two states.)\n",
        "1. **[4 pts]** Why do all the plots corresponding to full Monte Carlo look the same?\n",
        "1. **[4 pts]** Why do the plots corresponding to full Monte Carlo have the shape they do?\n",
        "1. **[4 pts]** How would the results above change (at high level, not in terms of precise plots) if the behaviour policy $\\mu$ would select action $a$ more often (e.g., $\\mu = 0.8$)?  How would the results change if the behaviour would select $a$ less often (e.g., $\\mu = 0.2$)?\n",
        "1. **[4 pts]** Consider again the orginal experiment, where data is gathered under uniformly random behaviour policy. What would the updates to the vectors $w$ be under the $L_\\infty$ norm? You can either run the experiment or give the closed-form update in an equation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0pm2-4rvB49"
      },
      "source": [
        "## Put answers to Q9 in this cell:\n",
        "1. _...answer here..._\n",
        "2. _...answer here..._\n",
        "3. _...answer here..._\n",
        "4. _...answer here..._\n",
        "5. _...answer here..._\n",
        "6. _...answer here..._\n",
        "7. _...answer here..._\n",
        "8. _...answer here..._"
      ]
    }
  ]
} {
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_assignment_2_2021.ipynb",
      "provenance": [
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1582541397384
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1581969444858
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1581964637124
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1581957222796
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2.ipynb?workspaceId=mtthss:ucl::citc",
          "timestamp": 1581683857481
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2.ipynb",
          "timestamp": 1581608852108
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_2.ipynb",
          "timestamp": 1580904796770
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_2.ipynb",
          "timestamp": 1548782473207
        },
        {
          "file_id": "1t1PIXqa3m-irLvQ_fQ7qgaBZeTNqiG2v",
          "timestamp": 1542801802962
        },
        {
          "file_id": "1Ldj742iIDtvjYKKwENvrpTQ3Hm2wrqIg",
          "timestamp": 1517862636703
        },
        {
          "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
          "timestamp": 1517660129183
        },
        {
          "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
          "timestamp": 1517174839485
        },
        {
          "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
          "timestamp": 1515086437469
        },
        {
          "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
          "timestamp": 1511098107887
        }
      ],
      "collapsed_sections": [
        "qB0tQ4aiAaIu"
      ],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook",
        "kind": "private"
      },
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        # RL homework 2\n",
        "**Due date: 30th March 2021, 4:00pm **\n",
        "\n",
        "Name: ***put your name here***\n",
        "\n",
        "Student number: ***put your student number here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sns0IKYNtsA"
      },
      "source": [
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<student_id>_ucldm_rl2.ipynb`** before the deadline above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "## Context\n",
        "\n",
        "In this assignment, we will take a first look at learning algorithms for sequential decision problems.\n",
        "\n",
        "## Background reading\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 3 - 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use Python to implement several reinforcement learning algorithms.\n",
        "\n",
        "You will then run these algorithms on a few problems, to understand their properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB0tQ4aiAaIu"
      },
      "source": [
        "### Import Useful Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "source": [
        "import matplotlib.collections as mcoll\n",
        "import matplotlib.path as mpa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NDhSYfSDcCC"
      },
      "source": [
        "### Set options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "source": [
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-colorblind')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrRR76eAd6u"
      },
      "source": [
        "### Some grid worlds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "source": [
        "W = -100  # wall\n",
        "G = 100  # goal\n",
        "\n",
        "GRID_LAYOUT = np.array([\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W],\n",
        "  [W, W, 0, W, W, W, W, W, W, 0, W, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, G, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, W, 0, 0, 0, 0, 0, 0, 0, 0, W, W],\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W]\n",
        "])\n",
        "\n",
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = GRID_LAYOUT\n",
        "    self._start_state = (2, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "\n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(self, obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "    \n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    reward = self._layout[new_y, new_x]\n",
        "    if self._layout[new_y, new_x] == W:  # wall\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = -1.\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "\n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 10*np.random.normal(0, width - new_x + new_y)\n",
        "\n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()\n",
        "\n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout != W, interpolation=\"nearest\", cmap='pink')\n",
        "    plt.gca().grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(9, 2, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmHhbuuk8bPm"
      },
      "source": [
        "SMALL_GRID_LAYOUT = np.array([\n",
        "  [W, W, W, W, W],\n",
        "  [W, W, 3, W, W],\n",
        "  [W, W, 0, W, W],\n",
        "  [W, 1, 0, W, W],\n",
        "  [W, W, 0, W, W],\n",
        "  [W, W, 0, 4, W],\n",
        "  [W, W, W, W, W]\n",
        "])\n",
        "\n",
        "\n",
        "def plot_small_grid(version=1):\n",
        "  plt.imshow(SMALL_GRID_LAYOUT < -1, interpolation='nearest', cmap='pink_r',\n",
        "             vmin=-0.2, vmax=1.2)\n",
        "  if version == 1:\n",
        "    plt.text(2, 1, '$+3$', ha='center', va='center', fontsize=12)\n",
        "    plt.text(1, 3, '$+1$', ha='center', va='center', fontsize=12)\n",
        "    plt.text(3, 5, '$+4$', ha='center', va='center', fontsize=12)\n",
        "  else:\n",
        "    plt.text(2, 1, '$+2$', ha='center', va='center', fontsize=12)\n",
        "    plt.text(1, 3, '$+1$', ha='center', va='center', fontsize=12)\n",
        "    plt.text(3, 5, '$+5$', ha='center', va='center', fontsize=12)\n",
        "  plt.text(2, 3, '$S$', ha='center', va='center', fontsize=12)\n",
        "  h, w = SMALL_GRID_LAYOUT.shape\n",
        "  for r in np.arange(0.5, h):\n",
        "    plt.plot([-0.5, w - 0.5], [r, r], '-k', lw=3, alpha=0.4)\n",
        "  for c in np.arange(0.5, w):\n",
        "    plt.plot([c, c], [-0.5, h - 0.5], '-k', lw=3, alpha=0.4)\n",
        "  plt.xticks([]); plt.yticks([]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOu9RZY3AkF1"
      },
      "source": [
        "### Helper functions\n",
        "(You should not have to change, or even look at, these.  Do run the cell to make sure the functions are loaded though.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EttQGJ1n5Zn"
      },
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = grid.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += reward\n",
        "    return mean_reward/float(number_of_steps)\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(grid, values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values - 1000*(grid<0), interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(grid, action_values, vmin=-5, vmax=5):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(4, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(grid, q[..., a], vmin=vmin, vmax=vmax)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(4, 3, 5)\n",
        "  v = np.max(q, axis=-1)\n",
        "  plot_values(grid, v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "  \n",
        "  # Plot arrows:\n",
        "  plt.subplot(4, 3, 11)\n",
        "  plot_values(grid, grid==0, vmax=1)\n",
        "  for row in range(len(grid)):\n",
        "    for col in range(len(grid[0])):\n",
        "      if grid[row][col] == 0:\n",
        "        argmax_a = np.argmax(q[row, col])\n",
        "        if argmax_a == 0:\n",
        "          x = col\n",
        "          y = row + 0.5\n",
        "          dx = 0\n",
        "          dy = -0.8\n",
        "        if argmax_a == 1:\n",
        "          x = col - 0.5\n",
        "          y = row\n",
        "          dx = 0.8\n",
        "          dy = 0\n",
        "        if argmax_a == 2:\n",
        "          x = col\n",
        "          y = row - 0.5\n",
        "          dx = 0\n",
        "          dy = 0.8\n",
        "        if argmax_a == 3:\n",
        "          x = col + 0.5\n",
        "          y = row\n",
        "          dx = -0.8\n",
        "          dy = 0\n",
        "        plt.arrow(x, y, dx, dy, width=0.02, head_width=0.4, head_length=0.4, length_includes_head=True, fc='k', ec='k')\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "\n",
        "def parameter_study(parameter_values, parameter_name,\n",
        "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])\n",
        "\n",
        "\n",
        "def colorline(x, y, z):\n",
        "    \"\"\"\n",
        "    Based on:\n",
        "    http://nbviewer.ipython.org/github/dpsanders/matplotlib-examples/blob/master/colorline.ipynb\n",
        "    http://matplotlib.org/examples/pylab_examples/multicolored_line.html\n",
        "    Plot a colored line with coordinates x and y\n",
        "    Optionally specify colors in the array z\n",
        "    Optionally specify a colormap, a norm function and a line width\n",
        "    \"\"\"\n",
        "    segments = make_segments(x, y)\n",
        "    lc = mcoll.LineCollection(segments, array=z, cmap=plt.get_cmap('copper_r'),\n",
        "                              norm=plt.Normalize(0.0, 1.0), linewidth=3)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.add_collection(lc)\n",
        "    return lc\n",
        "\n",
        "\n",
        "def make_segments(x, y):\n",
        "    \"\"\"\n",
        "    Create list of line segments from x and y coordinates, in the correct format\n",
        "    for LineCollection: an array of the form numlines x (points per line) x 2 (x\n",
        "    and y) array\n",
        "    \"\"\"\n",
        "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "    return segments\n",
        "\n",
        "\n",
        "def plotting_helper_function(_x, _y, title=None, ylabel=None):\n",
        "  z = np.linspace(0, 0.9, len(_x))**0.7\n",
        "  colorline(_x, _y, z)\n",
        "  plt.plot(0, 0, '*', color='#000000', ms=20, alpha=0.7, label='$w^*$')\n",
        "  plt.plot(1, 1, '.', color='#ee0000', alpha=0.7, ms=20, label='$w_0$')\n",
        "  min_y, max_y = np.min(_y), np.max(_y)\n",
        "  min_x, max_x = np.min(_x), np.max(_x)\n",
        "  min_y, max_y = np.min([0, min_y]), np.max([0, max_y])\n",
        "  min_x, max_x = np.min([0, min_x]), np.max([0, max_x])\n",
        "  range_y = max_y - min_y\n",
        "  range_x = max_x - min_x\n",
        "  max_range = np.max([range_y, range_x])\n",
        "  plt.arrow(_x[-3], _y[-3], _x[-1] - _x[-3], _y[-1] - _y[-3], color='k',\n",
        "            head_width=0.04*max_range, head_length=0.04*max_range,\n",
        "            head_starts_at_zero=False)\n",
        "  plt.ylim(min_y - 0.2*range_y, max_y + 0.2*range_y)\n",
        "  plt.xlim(min_x - 0.2*range_x, max_x + 0.2*range_x)\n",
        "  ax = plt.gca()\n",
        "  ax.ticklabel_format(style='plain', useMathText=True)\n",
        "  plt.legend(loc=2)\n",
        "  plt.xticks(rotation=12, fontsize=10)\n",
        "  plt.yticks(rotation=12, fontsize=10)\n",
        "  plt.locator_params(nbins=3)\n",
        "  if title is not None:\n",
        "    plt.title(title, fontsize=20)\n",
        "  if ylabel is not None:\n",
        "    plt.ylabel(ylabel, fontsize=20)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# Section 1: Tabular RL _(60 pts in total for the whole section)_\n",
        "\n",
        "In this section, observations will be states in the environment, so the agent state, environment state, and observation will all be the same, and we will use the word `state` interchangably with `observation`.  You will implement agents, which should be in pure Python - so you cannot use JAX/TensorFlow/PyTorch to compute gradients. Using `numpy` is fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53WqBnBywTHm"
      },
      "source": [
        "## Q1: A small grid world _(10 pts in total)_\n",
        "\n",
        "Consider the grid MDP below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1rDz0bP5J4m"
      },
      "source": [
        "plot_small_grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5ywpx298scu"
      },
      "source": [
        "The agent starts at the state marked $S$, and  can move up, left, right, or down.  Whenever it enters a cell with a positive number, the agent receives that reward and the episode terminates.\n",
        "### Q1.1 [**3 pts**]\n",
        "Use the code below to plot the value of the optimal policy from the starting state, $v_*(S)$, as a function of the discount factor $\\gamma$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prj9_fvXdeET"
      },
      "source": [
        "def v(discount):\n",
        "  return -1.  # This is wrong, replace it with the appropriate calculation\n",
        "\n",
        "# Don't change this function\n",
        "def plot_v():\n",
        "  discounts = np.arange(0, 1.001, 0.01)\n",
        "  plt.plot(discounts, [v(g) for g in discounts])\n",
        "  plt.xlabel('$\\gamma$')\n",
        "  plt.ylabel('$v_*(S)$')\n",
        "\n",
        "plot_v()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Igl6g0Sgw2v"
      },
      "source": [
        "### Q1.2 [**2 pts**]\n",
        "\n",
        "Specify the optimal policy, as a function of $\\gamma$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcinsTpmmmkW"
      },
      "source": [
        "### Answer\n",
        "\n",
        "_(Double click here to write your answer)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywAFQua4hL-U"
      },
      "source": [
        "### Q1.3 [**2 pts**]\n",
        "Consider the variation of this problem shown in the next plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPkHt2P8hsBc"
      },
      "source": [
        "plot_small_grid(version=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFB_JRgBhwB9"
      },
      "source": [
        "Suppose a behavioural scientist was doing an experiment where they gave rewards to an animal.  Suppose the setting was as depicted above, where the rewards for instance correspond to food or some other kind of treat (e.g., 5 food pellets at the location marked $+5$). It turns out that, after repeatedly exploring the grid, the animal seems to prefer going up to the reward of $+2$.\n",
        "\n",
        "Prove that in the MDP depicted above no scalar discount $\\gamma \\in [0, 1]$ exists for which the optimal policy is to go to $+2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzF3dp9Piwsu"
      },
      "source": [
        "### Answer\n",
        "\n",
        "_(Double click here to write your answer)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvGKvIYRiLvq"
      },
      "source": [
        "### Q1.4 [**3 pts**]\n",
        "In the same setting as the previous question, now consider the following generalised definition of a Monte Carlo return\n",
        "\\begin{align*}\n",
        "G_t\n",
        "& = R_{t+1} + f(R_{t+2} + f(R_{t+3} + f(\\ldots))) \\\\\n",
        "& = R_{t+1} + f(G_{t+1}) \\,,\n",
        "\\end{align*}\n",
        "In this formulation, we get standard discounting when we define $f(x) = \\gamma x$. Consider the following alternative where instead of multiplying with a factor $\\gamma$, we raise the value to the power: $f(x) = x^\\gamma$.  Does this mathematical model better explain the observed behaviour, in the sense that a $\\gamma$ exists for which the optimal policy goes to $+2$?  If so, give such a value for $\\gamma$, and prove that the policy of going to $+2$ is optimal with that choice.  If not, prove that no such $\\gamma$ exists.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLGbEPzJi0hH"
      },
      "source": [
        "### Answer\n",
        "\n",
        "_(Double click here to write your answer)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGTgvDm6wTRg"
      },
      "source": [
        "# Implementing algorithms\n",
        "\n",
        "**Minimal agent interface**:\n",
        "\n",
        "Each agent should implement the following methods:\n",
        "\n",
        "### `__init__(self, number_of_actions, number_of_states, initial_observation, ...)`:\n",
        "The constructor will provide the agent the number of actions, number of states, and the initial state. You can get the initial state by first instatiating an environment, using `grid = Grid()`, and then calling `grid.get_obs()`. The constructor may accept additional arguments (denoted here by the ellipsis `...`) depending on the specific algorithm implemented by the agent.\n",
        "\n",
        "### `step(self, reward, discount, next_state)`:\n",
        "\n",
        "The step method of an agent should update its internals, in whatever way is appropriate for its learning algorithm, and return a new action to be executed in the environment.\n",
        "\n",
        "When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_state` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_state})$\" in the update, because $\\gamma = 0$ (for whatever definition of $v$ is appropriate---for instance, $v(s)$ could be defined in terms of action values estimates that we are learning, for instance by $v(s) = \\max_a q(s, a)$).  Therefore, the end of an episode can be seamlessly handled with the same step function.\n",
        "\n",
        "**Note on the initial action**:\n",
        "\n",
        "Some algorithms (Q-learning, Sarsa) need to remember the last action in order to update its value when they see the next state and reward.  In the constructor (`__init__`), make sure you set the initial action to zero, e.g.,\n",
        "```\n",
        "def __init__(...):\n",
        "  (...)\n",
        "  self._last_action = 0\n",
        "  (...)\n",
        "```\n",
        "In our experiments the helper functions above will execute the action `0` (which corresponds to `up` in the grid world) as the initial action to begin the run loop of the experiment.  This initial action is only executed once, and the beginning of the very first episode---not at the beginning of each episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Z5IgXfU2Qw"
      },
      "source": [
        "### A random agent\n",
        "\n",
        "Below we show a reference implementation of a simple random agent, implemented according to the interface above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf64o3b3U6A4"
      },
      "source": [
        "class Random(object):\n",
        "\n",
        "  def __init__(self, number_of_actions, number_of_states, initial_state):\n",
        "    self._number_of_actions = number_of_actions\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    next_action = np.random.randint(self._number_of_actions)\n",
        "    return next_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaGeLcsvixmt"
      },
      "source": [
        "### The grid\n",
        "\n",
        "The cell below shows the `Grid` environment that we will use in this section. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-100` for bumping into a wall, `+100` for reaching the goal, and `-1` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlFuWFzIi5uB"
      },
      "source": [
        "grid = Grid()\n",
        "grid.plot_grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8oKd0oyvNcH"
      },
      "source": [
        "\n",
        "## Q2: Implement TD learning **[5 pts]**\n",
        "Implement an agent that acts randomly, and _on-policy_ estimates state values $v(s)$, using one-step TD learning with step size $\\alpha=0.1$.\n",
        "\n",
        "In addition to the base interface, also implement a property `state_values(self)` returning the vector of all state values (one value per state).\n",
        "\n",
        "You should be able to use the `__init__` as provided below, so you just have to implement the `step` function.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the TD error when the first transition has occurred.  \n",
        "\n",
        "**Hint**: in the `step` you similarly will want to store the previous state to be able to compute the next TD error on the next step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yKt1qYYjWTR"
      },
      "source": [
        "class RandomTD(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
        "    self._values = np.zeros(number_of_states)\n",
        "    self._state = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "\n",
        "  @property\n",
        "  def state_values(self):\n",
        "    return self._values\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaMmp1lDgpUG"
      },
      "source": [
        "### Run the next cell to run the `RandomTD` agent on a grid world.\n",
        "\n",
        "If everything worked as expected, the plot below will show the estimates state values under the random policy. This includes values for unreachable states --- on the walls and on the goal (we never actually reach the goal --- rather, the episode terminates on the transition to the goal.  The values on the walls and goal are, and will always remain, zero (shown in orange below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0ZoYwgZfho2"
      },
      "source": [
        "# Do not modify this cell.\n",
        "agent = RandomTD(grid._layout.size, 4, grid.get_obs())\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "v = agent.state_values\n",
        "plot_values(GRID_LAYOUT, v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-300, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxc_Sx7og4JH"
      },
      "source": [
        "## Q3: Policy iteration **[5 pts]**\n",
        "We used TD to do policy evaluation for the random policy on this problem.  Consider doing policy improvement, by taking the greedy policy with respect to a one-step look-ahead.  For this, you may assume we have a true model, so for each state and for each action we can look at the value of the resulting state, and would then pick the action with the highest reward plus subsequent state value. In other words, you can assume we can use $q(s, a) = \\mathbb{E}[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s, A_t = a]$, where $v$ is the value function learned by TD as implemented. Then we consider the policy that picks the action with the highest action value $q(s, a)$. You do **not** have to implement this, just answer the following question.\n",
        "\n",
        "The above amounts to performing an iteration of policy evaluation and policy improvement.  If we repeat this process over and over again, and repeatedly evaluate the greedy policy and then perform an improvement step by picking the greedy policy, would the policy eventually become optimal?  Explain why or why not in at most three sentences.\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfA7ifHvO-M"
      },
      "source": [
        "\n",
        "## Q4: Implement a general Q-learning agent **[15 pts]**\n",
        "Implement a  **general Q-learning** agent that learns action values from experience.   The agent must act according to an $\\epsilon$-greedy policy over its action values.  It must be configurable so as to implement any of **Sarsa**, **Expected Sarsa**, **Q-learning**,  and **double Q-learning**.\n",
        "\n",
        "The `__init__` must accept two functions `target_policy` and `behaviour_policy` as arguments.   The function `behaviour_policy(action_values)` should map `action_values` to a single action. \n",
        "\n",
        "For instance, the random policy can be implemented as:\n",
        "```\n",
        "def behaviour_policy(action_values):\n",
        "  return np.random.randint(len(action_values))\n",
        "```\n",
        "\n",
        "The target policy is defined by a function `target_policy(action_values, action)`, which returns **a vector** with one probability per action.  The `action` argument is used for instance by Sarsa (where the target policy is a greedy distribution with its peak on the selected action).\n",
        "\n",
        "For instance, the random target policy is:\n",
        "```\n",
        "def target_policy(action_values, unused_action):\n",
        "  number_of_actions = len(action_values)\n",
        "  return np.ones((number_of_actions,))/number_of_actions\n",
        "```\n",
        "\n",
        "The `__init__` must also accept  a `double` boolean flag. Note that this is compatible with any choice of `target_policy` and `behaviour_policy`. For instance, if the `target_policy` is the policy described above for Sarsa and `double=True`, the algorithm should implement **double Sarsa**. Note that we then need two action-value functions.\n",
        "\n",
        "You should be able to use the `__init__` as provided below, so you just have to implement the `step` function.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the update on the first transition in the agent's lifetime.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEuPtZVwjtq2"
      },
      "source": [
        "class GeneralQ(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state,\n",
        "               target_policy, behaviour_policy, double, step_size=0.1):\n",
        "    # Settings.\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._target_policy = target_policy\n",
        "    self._double = double\n",
        "    # Initial state.\n",
        "    self._s = initial_state\n",
        "    # Tabular q-estimates.\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    if double:\n",
        "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
        "    # The first action in an agent's lifetime is always 0(=up) in our setup.\n",
        "    self._last_action = 0\n",
        "\n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return (self._q + self._q2)/2 if self._double else self._q\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkk8sMxE0N4"
      },
      "source": [
        "## Experiment 1: Run the cells below\n",
        "\n",
        "This will train Q-learning, Sarsa, Expected Sarsa, and double Q-learning agents on the deterministic version of the Grid problem.\n",
        "\n",
        "The agents will be trained  with a step size $\\alpha=\\frac{1}{10}$ and $\\epsilon$-greedy behaviour, with $\\epsilon=\\frac{1}{4}$.\n",
        "\n",
        "The plots will show action values for each of the actions, as well as a state value defined by $v(s) = \\max_a q(s, a)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlsvLKxYRA27"
      },
      "source": [
        "# Agent settings.\n",
        "# Do not modify this cell.\n",
        "epsilon = 0.25\n",
        "step_size = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "GsNBHNZtHCPe"
      },
      "source": [
        "# Q-learning\n",
        "# Do not modify this cell.\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  return np.eye(len(q))[np.argmax(q)]\n",
        "\n",
        "learned_qs = []\n",
        "for _ in range(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=False, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-20, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ26VmlsSSmw"
      },
      "source": [
        "# Sarsa\n",
        "# Do not modify this cell.\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  return np.eye(len(q))[a]\n",
        "\n",
        "learned_qs = []\n",
        "for _ in range(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=False, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-20, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mti_FtEeSaqH"
      },
      "source": [
        "# Expected Sarsa\n",
        "# Do not modify this cell.\n",
        "grid = Grid()\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  greedy = np.eye(len(q))[np.argmax(q)]\n",
        "  return greedy - greedy*epsilon + epsilon/4 \n",
        "\n",
        "learned_qs = []\n",
        "for _ in range(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=False, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-20, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DwHqxeZ1rXa"
      },
      "source": [
        "# Double Q-learning\n",
        "# Do not modify this cell.\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  # Place equal probability on all actions that achieve the `max` value.\n",
        "  # This is equivalent to `return np.eye(len(q))[np.argmax(q)]` for Q-learning\n",
        "  # But results in slightly lower variance updates for double Q.\n",
        "  max_q = np.max(q)\n",
        "  pi = np.array([1. if qi == max_q else 0. for qi in q])\n",
        "  return pi / sum(pi)\n",
        "\n",
        "learned_qs = []\n",
        "for _ in range(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=True, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-50, vmax=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGptHwE23lmP"
      },
      "source": [
        "## Q5: Analyse results _(10 pts in total)_\n",
        "\n",
        "Consider the greedy policy with respect to the estimated values learned by each of the four agents.\n",
        "\n",
        "**[5 pts]** How and why do the policies found by Q-learning, Sarsa, Expected Sarsa, and double Q-learning differ? Explain notable qualitative differences in at most four sentences.\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "**[5 pts]** Which of the algorithms, out of Q-learning, Sarsa, Expected Sarsa, and double Q-learning with the learning parameters (exploration, step size) as discussed above, will *in general* yield higher returns on average during learning?  You are allowed to specify a partial (rather than a full) ordering over the algorithms, but try to be as specfic as you can. Explain your answer in at most four sentences.\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK0ExHgqn8eQ"
      },
      "source": [
        "## Experiment 2: Run the cells below\n",
        "\n",
        "This will train Q-learning and double Q-learning agents on the stochastic version of the Grid problem.\n",
        "\n",
        "In the noisy version, a zero-mean Gaussian is added to all rewards.  \n",
        "\n",
        "The variance of this noise is higher the further to the left you go, and the further down.\n",
        "\n",
        "The agents will be trained  with a step size $\\alpha=\\frac{1}{10}$ and $\\epsilon$-greedy behaviour, with $\\epsilon=\\frac{1}{4}$.\n",
        "\n",
        "The plots will show action values for each of the actions, as well as a state value defined by $v(s) = \\max_a q(s, a)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LfLSB5Gn3Q6"
      },
      "source": [
        "# Agent settings.\n",
        "# Do not modify this cell.\n",
        "epsilon = 0.25\n",
        "step_size = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml8AmcN3nrvJ"
      },
      "source": [
        "# Q-learning and double Q-learning.\n",
        "# Do not modify this cell.\n",
        "\n",
        "def target_policy(q, a):\n",
        "  max_q = np.max(q)\n",
        "  pi = np.array([1. if qi == max_q else 0. for qi in q])\n",
        "  return pi / sum(pi)\n",
        "\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "\n",
        "mean_reward_q_learning = []\n",
        "mean_reward_double_q_learning = []\n",
        "for _ in range(30):\n",
        "  grid = Grid(noisy=True)\n",
        "  q_agent = GeneralQ(\n",
        "      grid._layout.size, 4, grid.get_obs(),\n",
        "      target_policy, behaviour_policy, double=False, step_size=step_size)\n",
        "  dq_agent = GeneralQ(\n",
        "      grid._layout.size, 4, grid.get_obs(),\n",
        "      target_policy, behaviour_policy, double=True, step_size=step_size)\n",
        "  mean_reward_q_learning.append(run_experiment(grid, q_agent, int(2e5)))\n",
        "  mean_reward_double_q_learning.append(run_experiment(grid, dq_agent, int(2e5)))\n",
        "\n",
        "plt.violinplot([mean_reward_q_learning, mean_reward_double_q_learning])\n",
        "plt.xticks([1, 2], [\"Q-learning\", \"Double Q-learning\"], rotation=60, size=12)\n",
        "plt.ylabel(\"average reward during learning\", size=12)\n",
        "ax = plt.gca()\n",
        "ax.grid(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Jj_RoHbxfF"
      },
      "source": [
        "## Q6: Analyse results **[8 pts]**\n",
        "\n",
        "Which among Q-learning and Double Q-learning has a higher average reward? Explain why, discussing at least a) the dynamics of the algorithm, b) how this affects behaviour, and c) why the behaviour yields higher rewards for one algorithm rather than the other. Be concise.\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr71oDDAsxTT"
      },
      "source": [
        "## Q7: Target Q-learning **[7 pts]**\n",
        "\n",
        "Consider a new algorithm which we will refer to as **target Q-learning**. The target Q-learning algorithm applies Q-learning updates in the form \n",
        "$\\ \\ q(S_t, A_t) \\leftarrow R_{t+1} + \\gamma \\max_a q'(S_{t+1}, a) \\ \\ $  \n",
        "where the values $q'$ have been pre-trained by running $\\epsilon$-greedy Expected Sarsa, and are then held fixed throughout training.\n",
        "\n",
        "We can now imagine now to train both Q-learning and target Q-learning on the deterministic Grid problem.  In both Q-learning and target Q-learning the behaviour policy will select actions based on the same $\\epsilon$-greedy policy that was used to pre-train the $q'$ values, but using the current action-value estimates (not the fixed target values $q'$).\n",
        "\n",
        "Explain concisely which of the two algorithms will perform better and why. (If you're tempted to answer 'it depends', be sure to be clear what you think it depends on, and why. Don't be vague: point could be subtracted for including irrelevant or false statements, even if the correct answer is also given.)\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R58NP87jbM"
      },
      "source": [
        "# Section 2: Off-policy Bellman operators with function approximation\n",
        "# _(40 pts total for the whole section)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIyALFeoiov9"
      },
      "source": [
        "## Q8: Bellman operator for prediction **[10 pts]**\n",
        "\n",
        "We are going to implement a Bellman operator for a simple MDP. You should:\n",
        "\n",
        "\n",
        "1. Write a value function `v(w, x)` that outputs a linear value estimate when we have weights `w` (a numpy vector) and feature vector `x` (a numpy vector of the same size).\n",
        "2. Write an operator function `T(w, pi, mu, l, g)` that takes weights `w`, a target policy `pi`, a behaviour policy `mu`, a trace parameter `l`, and a discount `g`, and outputs an off-policy-corrected lambda-return.  For this question, implement the standard importance-weighted per-decision lambda-return. There will only be two actions, with the same policy in each state, so we can define `pi` to be a number which is the target probability of selecting action `a` in any state (s.t. `1 - pi` is the probability of selecting `b`), and similarly for the behaviour `mu`.\n",
        "3. Write an expected weight update, that uses the above two functions to compute the **expected** weight update.  The expectation should take into account the probabilities of actions in the future, as well as the steady-state (=long-term) probability of being in a state.  The step size of the update should be $\\alpha=0.1$.\n",
        "\n",
        "There are two states, $s_1$ and $s_2$.  All rewards are zero, and therefore can be ignored.  The state features $x_1 = x(s_1)$ and $x_2 = x(s_2)$ for the two states are $x_1 = [1, 1]^{\\top}$ and $x_2 = [2, 1]^{\\top}$.  In each state, there are two actions, $a$ and $b$.  Action $a$ always transitions to state $s_1$, action $b$ always transitions to state $s_2$.\n",
        "\n",
        "![MDP](https://hadovanhasselt.files.wordpress.com/2020/02/mdp.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEpxlyUtrj3i"
      },
      "source": [
        "# state features (do not change)\n",
        "x1 = np.array([1., 1.])\n",
        "x2 = np.array([2., 1.])\n",
        "\n",
        "def v(w, x):\n",
        "  pass\n",
        "\n",
        "def T(w, pi, mu, l, g):\n",
        "  pass\n",
        "\n",
        "def expected_update(w, pi, mu, l, g, lr):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U42IsCPW0KGY"
      },
      "source": [
        "##Experiment 3: run the cell below\n",
        "The cell below runs an experiment, across different target policies and trace parameters $\\lambda$.\n",
        "\n",
        "The plots below the cell will show how the weights move within the 2-dimensional weight space, starting from $w_0 = [1, 1]^{\\top}$ (shown as red dot).  The optimal solution $w_* = [0, 0]^{\\top}$ is also shown (as black star)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTFFpQSX0Eaj"
      },
      "source": [
        "def generate_ws(w, pi, mu, l, g):\n",
        "  \"\"\"Apply the expected update 1000 times\"\"\"\n",
        "  ws = [w]\n",
        "  for _ in range(1000):\n",
        "    w = w + expected_update(w, pi, mu, l, g, lr=0.1)\n",
        "    ws.append(w)\n",
        "  return np.array(ws)\n",
        "\n",
        "mu = 0.5  # behaviour\n",
        "g = 0.99  # discount\n",
        "\n",
        "lambdas = np.array([0, 0.8, 0.9, 0.95, 1.])\n",
        "pis = np.array([0., 0.1, 0.2, 0.5, 1.])\n",
        "\n",
        "fig = plt.figure(figsize=(22, 17))\n",
        "fig.subplots_adjust(wspace=0.25, hspace=0.3)\n",
        "\n",
        "for r, pi in enumerate(pis):\n",
        "  for c, l in enumerate(lambdas):\n",
        "    plt.subplot(len(pis), len(lambdas), r*len(lambdas) + c + 1)\n",
        "    w = np.ones_like(x1)\n",
        "    ws = generate_ws(w, pi, mu, l, g)\n",
        "    title = '$\\\\lambda={:1.3f}$'.format(l) if r == 0 else None\n",
        "    ylabel = '$\\\\pi={:1.1f}$'.format(pi) if c == 0 else None\n",
        "    plotting_helper_function(ws[:, 0], ws[:, 1], title, ylabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KxL4o357_dt"
      },
      "source": [
        "## Q9: Analyse results (30 pts total)\n",
        "1. **[2 pts]** How many of the above 25 experiments diverge?\n",
        "1. **[4 pts]** Why are the results asymmetric across different $\\pi$?  In particular, explain why the results look different when comparing $\\pi = \\pi(a | \\cdot) = 0$ to $\\pi(a | \\cdot) = 1$.\n",
        "1. **[4 pts]** For which policies $\\pi$, is the true value function $v_{\\pi}$ representable in the above feature space (spanned by $x_1, x_2$).\n",
        "1. **[4 pts]** For which combination of $\\pi(a)$ and $\\lambda$ does the expected update (with uniform random behaviour) converge? (Do not limit the answer to the subset of values in the plots above, but to all choices of $\\lambda$ and $\\pi$, but do restrict yourself to state-less policies, as above, for which the action probabilities are equal in the two states.)\n",
        "1. **[4 pts]** Why do all the plots corresponding to full Monte Carlo look the same?\n",
        "1. **[4 pts]** Why do the plots corresponding to full Monte Carlo have the shape they do?\n",
        "1. **[4 pts]** How would the results above change (at high level, not in terms of precise plots) if the behaviour policy $\\mu$ would select action $a$ more often (e.g., $\\mu = 0.8$)?  How would the results change if the behaviour would select $a$ less often (e.g., $\\mu = 0.2$)?\n",
        "1. **[4 pts]** Consider again the orginal experiment, where data is gathered under uniformly random behaviour policy. What would the updates to the vectors $w$ be under the $L_\\infty$ norm? You can either run the experiment or give the closed-form update in an equation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0pm2-4rvB49"
      },
      "source": [
        "## Put answers to Q9 in this cell:\n",
        "1. _...answer here..._\n",
        "2. _...answer here..._\n",
        "3. _...answer here..._\n",
        "4. _...answer here..._\n",
        "5. _...answer here..._\n",
        "6. _...answer here..._\n",
        "7. _...answer here..._\n",
        "8. _...answer here..._"
      ]
    }
  ]
}"
